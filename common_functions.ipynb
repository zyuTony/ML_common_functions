{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923c5092",
   "metadata": {},
   "source": [
    "### Overview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d81df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_num_cols(data, top_val_lim=0):\n",
    "    '''\n",
    "    Find and returns the categorical and numerical variables. Pring out the \\\n",
    "    top unique values of the categorical variables.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas df): the data\n",
    "    top_val_lim (int): how many top unique values to show for every categorical \\\n",
    "    variables\n",
    "    \n",
    "    Returns:\n",
    "    cat_cols: names of the categorical variables\n",
    "    num_cols: names of the numerical variables\n",
    "    '''\n",
    "    # Numerical and Categorical variables\n",
    "    cat_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    num_cols = data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    print(\"\\nCategorical variables:\\n\", cat_cols, \"\\n\")\n",
    "    print(\"Numerical variables:\\n\", num_cols)\n",
    "    \n",
    "    if top_val_lim != 0:\n",
    "        print(\"\\nTop\", top_val_lim, \"unique value counts for Categorical variables:\")\n",
    "        for i in cat_cols:   \n",
    "            print(\"---\", i, \"---\")\n",
    "            print(data[i].value_counts()[:top_val_lim], \"\\n\")\n",
    "        \n",
    "    return cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c4927",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346af241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6bfa11",
   "metadata": {},
   "source": [
    "### Preposessing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71176b2",
   "metadata": {},
   "source": [
    "#### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90efc43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_check(data, show_obs=0, feat_show_zero=True):\n",
    "    '''\n",
    "    Print percentage missing for each feature, and the value counts of the number of features that observation are missing\n",
    "    \n",
    "    Parameters:\n",
    "    data (pandas df): data\n",
    "    show_obs: default 0 means print nothing. Print out observations with show_obs or more missing features.\n",
    "    feat_show_zero: whether showing features that doesn't have missing values\n",
    "    \n",
    "    Returns:\n",
    "    miss_obs (pd series): Value counts of the number of features that observation are missing\n",
    "    missing_feat (pd series): Feature with missing values and the percentage missing\n",
    "    '''\n",
    "    # Percentage missing for each feature\n",
    "    missing_feat = data.isnull().sum()/data.isnull().count()\n",
    "    if feat_show_zero == False:\n",
    "        missing_feat = missing_feat[missing_feat!=0]\n",
    "\n",
    "    print(\"Percentage of data missing for each feature:\")\n",
    "    print(missing_feat.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "    # Percentage missing for each observation\n",
    "    feature_num = len(data.columns)\n",
    "    miss_obs = data.isnull().sum(axis=1)\n",
    "    print(\"Count of the observations with one or more (#) missing features:\")\n",
    "    print(\"#    Count\")\n",
    "    print(miss_obs.value_counts().sort_index(), \"\\n\")\n",
    "    \n",
    "    if show_obs != 0:\n",
    "        print(\"Observation with \", show_obs, \"or more missing features:\")\n",
    "        print(data.iloc[miss_obs[miss_obs >= show_obs].index.tolist(),:])\n",
    "        return data.iloc[miss_obs[miss_obs >= show_obs].index.tolist(),:], missing_feat[missing_feat!=0]\n",
    "    \n",
    "    #return missing_feat[missing_feat!=0]\n",
    "\n",
    "\n",
    "def impute_cat_var(data):\n",
    "    '''\n",
    "    Impute categorical variable of the data\n",
    "    \n",
    "    Parameter:\n",
    "    data (pandas df): data\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    cat_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    missing_feat = data.isnull().sum()/data.isnull().count()\n",
    "    missing_feat = missing_feat[missing_feat!=0]\n",
    "    miss_obs = data.isnull().sum(axis=1)\n",
    "    miss_cat_cols=set(cat_cols).intersection(missing_feat.index.tolist())\n",
    "\n",
    "    for i in miss_cat_cols:\n",
    "        replace_val = data[i].mode().item()\n",
    "        data[i].fillna(replace_val, inplace=True)\n",
    "\n",
    "        \n",
    "def impute_num_var(data, mean_impute=False):\n",
    "    '''\n",
    "    Impute categorical variable of the data\n",
    "    \n",
    "    Parameter:\n",
    "    data (pandas df): data\n",
    "    mean_impute (Bool): whether impute with mean. Default is false (with median)\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    num_cols = data.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    missing_feat = data.isnull().sum()/data.isnull().count()\n",
    "    missing_feat = missing_feat[missing_feat!=0]\n",
    "    miss_obs = data.isnull().sum(axis=1)\n",
    "    miss_num_cols=set(num_cols).intersection(missing_feat.index.tolist())\n",
    "\n",
    "    for i in miss_num_cols:\n",
    "        replace_val = data[i].median()\n",
    "        if mean_impute == True:\n",
    "            replace_val = data[i].mean()\n",
    "        data[i].fillna(replace_val, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b7794",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da70ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(data, cols, method=\"zscore\", cap=False, \n",
    "                    winsor_quatile = 0.01, z_percent=0.01, manual_high=0, manual_low=0, plot=True):\n",
    "    '''\n",
    "    Cap outliers\n",
    "    \n",
    "    Parameter:\n",
    "    data (pandas df): data\n",
    "    cols (list): columns to cap\n",
    "    method (string): zscore, iqr, or winsor\n",
    "    cap (Bool): whether to cap or not\n",
    "    quantile (int): quantile for winsor\n",
    "    percent (int): percentage for z-score\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    if method == 'zscore':\n",
    "        z_score = stats.norm.ppf(1-z_percent)\n",
    "        for i in cols:\n",
    "            high = data[i].mean() + z_score*data[i].std()\n",
    "            low = data[i].mean() - z_score*data[i].std()\n",
    "            \n",
    "            # Draw plot if needed\n",
    "            if plot:\n",
    "                ax = sns.distplot(data[i])\n",
    "                sns.set_theme(style=\"whitegrid\")\n",
    "                ax.set(title=\"Distribution Plot for {}\".format(i))\n",
    "                ax.axvline(low, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvline(high, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvspan(low, high, color='blue', alpha=0.1, lw=0)\n",
    "                plt.show()\n",
    "            \n",
    "            # Cap if selected\n",
    "            if cap:  \n",
    "                data[i] = np.select([data[i] > high, data[i] < low], \n",
    "                                    [high, low], data[i])\n",
    "                print(\"Capped!\")\n",
    "                \n",
    "    elif method =='iqr':\n",
    "        for i in cols:\n",
    "            per25 = data[i].quantile(0.25)\n",
    "            per75 = data[i].quantile(0.75)\n",
    "            iqr = per75 - per25\n",
    "            high = per75 + 1.5 * iqr\n",
    "            low = per25 - 1.5 * iqr\n",
    "            \n",
    "            # Draw plot if needed\n",
    "            if plot:\n",
    "                ax = sns.boxplot(data[i])\n",
    "                sns.set_theme(style=\"whitegrid\")\n",
    "                ax.set(title=\"Boxplot for {}\".format(i))\n",
    "                ax.axvline(low, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvline(high, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvspan(low, high, color='blue', alpha=0.1, lw=0)\n",
    "                plt.show()\n",
    "            \n",
    "            # Cap if selected\n",
    "            if cap:  \n",
    "                data[i] = np.select([data[i] > high, data[i] < low], \n",
    "                                    [high, low], data[i])\n",
    "                print(\"Capped!\")\n",
    "                \n",
    "    elif method =='winsor':\n",
    "        for i in cols:\n",
    "            high = data[i].quantile(1-winsor_quatile)\n",
    "            low = data[i].quantile(winsor_quatile)\n",
    "            \n",
    "            # Draw plot if needed\n",
    "            if plot:\n",
    "                ax = sns.boxplot(data[i])\n",
    "                sns.set_theme(style=\"whitegrid\")\n",
    "                ax.set(title=\"Boxplot for {}\".format(i))\n",
    "                ax.axvline(low, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvline(high, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvspan(low, high, color='blue', alpha=0.1, lw=0)\n",
    "                plt.show()\n",
    "            \n",
    "            # Cap if selected\n",
    "            if cap:  \n",
    "                data[i] = np.select([data[i] > high, data[i] < low], \n",
    "                                    [high, low], data[i])\n",
    "                print(\"Capped!\")\n",
    "    \n",
    "    elif method=='manual':\n",
    "         for i in cols:\n",
    "            high = manual_high\n",
    "            low = manual_low\n",
    "            \n",
    "            # Draw plot if needed\n",
    "            if plot:\n",
    "                ax = sns.boxplot(data[i])\n",
    "                sns.set_theme(style=\"whitegrid\")\n",
    "                ax.set(title=\"Boxplot for {}\".format(i))\n",
    "                ax.axvline(low, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvline(high, linestyle=\"--\", color=\"blue\")\n",
    "                ax.axvspan(low, high, color='blue', alpha=0.1, lw=0)\n",
    "                plt.show()\n",
    "            \n",
    "            # Cap if selected\n",
    "            if cap:  \n",
    "                data[i] = np.select([data[i] > high, data[i] < low], \n",
    "                                    [high, low], data[i])\n",
    "                print(\"Capped!\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Wrong entry for method. Select one in (zscore, iqr, winsor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da919da8",
   "metadata": {},
   "source": [
    "#### Linear Model Assumption Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab727bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa203549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vif_factors(X):\n",
    "    '''\n",
    "    Check VIF of independent variables\n",
    "    \n",
    "    Parameters:\n",
    "    X (2D array): the independent variables\n",
    "    \n",
    "    Returns\n",
    "    vif_factors: the VIF of the independent variables\n",
    "    '''\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    X_matrix = X.values\n",
    "    vif = [variance_inflation_factor(X_matrix,i) for i in range(X_matrix.shape[1])]\n",
    "    vif_factors = pd.DataFrame()\n",
    "    vif_factors[\"column\"] = X.columns\n",
    "    vif_factors[\"VIF\"] = vif\n",
    "    return vif_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50b9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_assum_check(resid, X, ts=False):\n",
    "    '''\n",
    "    Check the linear model assumptions.\n",
    "    \n",
    "    Parameters:\n",
    "    resid (1D array): the residual of the model. Y-Y_fitted\n",
    "    X (2D array): the values of all independent variables\n",
    "    ts (Boolean): whether the data is time series data. Default is False\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    # Set up the canvas\n",
    "    fig, axes = plt.subplots(nrows=2, figsize=(12,12))\n",
    "    plt.subplots_adjust(hspace=0.3, wspace = 0.5)\n",
    "    \n",
    "    # 1. Linear relationship: Residual vs. y fitted value\n",
    "    axes[0].scatter(y_pred, resid)\n",
    "    axes[0].grid()\n",
    "    axes[0].set_title('Residual vs. Y_Predicted')\n",
    "    axes[0].set_xlabel('Y_Predicted')\n",
    "    axes[0].set_ylabel('Residual')\n",
    "    \n",
    "    # 2. Residuals are Normally Distributed: QQ plot of residual\n",
    "    stats.probplot(resid, plot=axes[1])\n",
    "    \n",
    "    # 3. variance of errors is constant: Test for heteroscedasticity. \n",
    "    # p-val < 0.5 -> Error variances are not equal\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    bptest = het_breuschpagan(resid, X)[1]\n",
    "    print(\"Heteroscedasticity Test ------------------------\\n\")\n",
    "    print(\"The p value of Breuchpagen test is \", bptest, \".\\n\")\n",
    "    if bptest < 0.05:\n",
    "        print(\"Data has heteroscedasticity.\\n\")\n",
    "    else:\n",
    "        print(\"Data has homoscedasticity.\\n\")\n",
    "    \n",
    "    # 4. No multicollinearity: VIF factors for testing multicollinearity\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    X_matrix = X.values\n",
    "    vif = [variance_inflation_factor(X_matrix,i) for i in range(X_matrix.shape[1])]\n",
    "    vif_factors = pd.DataFrame()\n",
    "    vif_factors[\"column\"] = X.columns\n",
    "    vif_factors[\"VIF\"] = vif\n",
    "    \n",
    "    print(\"VIF---------------------------------------------\\n\")\n",
    "    print(vif_factors)\n",
    "    print(\"\\n(Usually a variable with VIF greater than 10 is considered to be troublesome.)\\n\")\n",
    "    print(\"------------------------------------------------\\n\")\n",
    "    \n",
    "    # 5. No autocorrelation of Errors: Durbin Watson test - No need for non-time series data\n",
    "    if ts==True:\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        print(durbin_watson(resid))\n",
    "        print((\"A value between 1.8 and 2.2 indicates no autocorrelation. \"\n",
    "              \"A value less than 1.8 indicates positive autocorrelation and a value greater\" \n",
    "              \" than 2.2 indicates negative autocorrelation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c63c0c",
   "metadata": {},
   "source": [
    "### Models\n",
    "Metrics and scoring: https://scikit-learn.org/stable/modules/model_evaluation.html <br>\n",
    "#### Libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03eede2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models import\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, ElasticNet\n",
    "from sklearn.linear_model import RidgeClassifier, Ridge  \n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# model_selection \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score  \n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, \\\n",
    "                            f1_score, precision_recall_curve, roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c68aaa",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ccdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a tuned model\n",
    "import joblib\n",
    "filename = 'tuned_model.sav'\n",
    "joblib.dump(model_grid, filename)\n",
    "\n",
    "# some time later...\n",
    "# load the model from disk\n",
    "# loaded_model is just like the model_grid\n",
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic Regression\n",
    "param_grid = {'penalty' : ['l1', 'l2'],\n",
    "              'C' : np.logspace(-4, 4, 20),\n",
    "              'solver' : ['liblinear']}\n",
    "\n",
    "# r2 and accuracy for default scoring\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring=\"f1\", verbose=2) \n",
    "logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "logreg_grid.predict(X_test,y_test)\n",
    "print(logreg_grid.best_params_, logreg_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95804e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC, SVR --- kernel=poly + gamma=1 + C=1  takes a long time to train\n",
    "param_grid = {'C': [0.1, 1, 10, 100], \n",
    "              'gamma': [1,0.1,0.01,0.001],\n",
    "              'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), param_grid, scoring='f1', refit=True, verbose=2)\n",
    "svm_grid.fit(X_train,y_train)\n",
    "\n",
    "svm_grid.predict(X_test,y_test)\n",
    "print(svm_grid.best_params_, svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb41207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38d4229d",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d8166",
   "metadata": {},
   "source": [
    "#### Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03cff20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "y_pred = model.predict_proba(X)[:,1]  # return prob of input being 1 \n",
    "\n",
    "def precision_recall_plot(y, y_pred):\n",
    "    '''\n",
    "    Plot precision-recall AUC\n",
    "    \n",
    "    Parameters:\n",
    "    y (array): the y value\n",
    "    y_pred (array): the predicted/fitted y values\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    '''\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(recall, precision, marker='.', label='Logistic')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    #ax.set_ylim([0.0, 1.05])\n",
    "    \n",
    "    base_precision = len(y[y==1]) / len(y)\n",
    "    ax.plot([0, 1], [base_precision, base_precision], 'g--', label='No Skill')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall-Curve', fontsize=16)\n",
    "    # ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "def roc_plot(y, y_pred):\n",
    "    '''\n",
    "    Plot ROC curve\n",
    "    \n",
    "    Parameters:\n",
    "    y (array): the y value\n",
    "    y_pred (array): the predicted/fitted y values\n",
    "    \n",
    "    Return:\n",
    "    None\n",
    "    '''\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, color=\"red\")\n",
    "    ax.plot([0, 1], [0, 1],'r--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver operating characteristic', fontsize=16)\n",
    "    #ax.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Optimal Threshold for precision-recall and roc\n",
    "precision, recall, threshold = precision_recall_curve(y_test, y_test_pred)\n",
    "fpr, tpr, threshold_roc = roc_curve(y_test, y_test_pred)\n",
    "# precision-recall\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "optimal_idx = np.argmax(fscore)\n",
    "threshold[optimal_idx]\n",
    "# roc\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = threshold_roc[optimal_idx]\n",
    "optimal_threshold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
